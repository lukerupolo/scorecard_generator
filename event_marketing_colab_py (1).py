# -*- coding: utf-8 -*-
"""event_marketing_colab.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vjxn1r-Ru8dRLwBEqQNL6n_lw8F4ZmYF
"""

# Cell 1: Install all required packages and configure your ngrok token
!pip install -q streamlit pyngrok pandas openpyxl requests numpy # Added requests

from pyngrok import ngrok, conf

# 🔑 Replace this with your actual ngrok Authtoken:
# This is only needed for running in Colab with ngrok, not for Streamlit Cloud deployment
!ngrok config add-authtoken 2xyaMUyxj5KM0l5tTL4evh59QfJ_7pmCiswiRKF93Hp3xGZcn

# Commented out IPython magic to ensure Python compatibility.
# %%writefile event_marketing_app.py
# # -------------------------------------------------------------
# # Streamlit – Event Marketing Analytics Suite
# # Modes:
# # 1) Generate template       → configure events & download blank workbook
# # 2) Final benchmarks        → upload completed workbook & download summaries
# # -------------------------------------------------------------
# from __future__ import annotations
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# from datetime import datetime, timedelta
# from io import BytesIO
# import requests # Added for API calls
# 
# # ---------------------------------------------------------
# # Helpers
# # ---------------------------------------------------------
# def format_span_labels(event_date: datetime) -> tuple[str, str]:
#     b_start = event_date - timedelta(days=7)
#     b_end   = event_date - timedelta(days=1)
#     a_end   = event_date + timedelta(days=6)
#     baseline = f"Baseline {b_start:%Y-%m-%d} → {b_end:%Y-%m-%d}"
#     actual   = f"Actual  {event_date:%Y-%m-%d} → {a_end:%Y-%m-%d}"
#     return baseline, actual
# 
# # ---------------------------------------------------------
# # New function: API Call (Replace with your actual API logic)
# # ---------------------------------------------------------
# def fetch_data_from_api(event_name: str, event_date: datetime, region: str, metric: str) -> float | None:
#     """
#     Fetches data for a specific event, region, and metric from an external API
#     using a retrieval URL and password.
# 
#     Args:
#         event_name: The name of the event.
#         event_date: The start date of the event.
#         region: The region/country code.
#         metric: The metric to fetch (e.g., "Video Views (VOD)").
# 
#     Returns:
#         The fetched data (float) if successful, otherwise None.
#     """
#     # 🔑 Using Streamlit Secrets for security in deployment
#     # In your deployed Streamlit app, you should store secrets
#     # in .streamlit/secrets.toml on Streamlit Cloud.
#     # See: https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app/secrets-management
#     # For local testing in Colab, you might temporarily hardcode or use environment variables.
# 
#     try:
#         retrieval_url = st.secrets["https://oneshot.ea.com/claim/da11cdd1ee186e9cad1edc761bac8ba540b2ea47c3139e2a921549ae61b76ab2"]
#         retrieval_password = st.secrets["9cb0bf4be92d49a1aa074f077dae44197395615d9397354e749545cd8588185d"]
#     except KeyError as e:
#         st.error(f"Missing required secret: {e}. Please configure your .streamlit/secrets.toml file.")
#         return None
#     except Exception as e:
#         st.error(f"Error accessing secrets: {e}")
#         return None
# 
#     # 🚨 DETERMINE HOW YOUR API EXPECTS PARAMETERS AND AUTHENTICATION 🚨
#     # This is the most critical part you need to adapt based on your API's documentation.
# 
#     # For demonstration, using a hypothetical GET request structure
#     # Replace with your actual API call implementation!
#     payload = {
#         "password": retrieval_password, # Some APIs put password in params
#         "event_name": event_name,
#         "event_date": event_date.strftime("%Y-%m-%d"), # Format date as required by API
#         "region": region,
#         "metric": metric
#     }
#     # headers = {} # Some APIs don't require special headers with password in params
#     try:
#          st.info(f"Attempting to fetch data for {metric} from API...") # Added debug info
#          api_response = requests.get(retrieval_url, params=payload) # Adjust method (get, post, etc.) as needed
# 
#          api_response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
#          data = api_response.json() # Assuming the API returns JSON
# 
#          # 🚨 REPLACE THIS WITH HOW YOUR API RESPONSE IS STRUCTURED 🚨
#          # You need to parse the JSON response to extract the value for the metric.
#          # Example: Assuming the API returns a JSON where the metric is a key
#          # For example: {"Video Views (VOD)": 12345.0, "Hours Watched (Streams)": 678.9}
#          if metric in data and isinstance(data[metric], (int, float)):
#               st.info(f"API fetched {metric} for {event_name} in {region}: {data[metric]}")
#               return float(data[metric])
#          else:
#               st.warning(f"API response for {metric} in {region} for {event_name} did not contain data for '{metric}' or format was incorrect.")
#               st.warning(f"API Response Content: {api_response.text}") # Added debug info
#               return None
# 
#     except requests.exceptions.RequestException as e:
#         st.error(f"API request failed for {metric} in {region} for {event_name}: {e}")
#         # You might want to print more details about the response for debugging:
#         if 'api_response' in locals():
#              st.error(f"Response status code: {api_response.status_code}")
#              st.error(f"Response content: {api_response.text}")
#         return None
#     except ValueError: # Handles case where response is not valid JSON
#          st.error(f"API response for {metric} in {region} for {event_name} was not valid JSON.")
#          if 'api_response' in locals():
#               st.error(f"Response content: {api_response.text}")
#          return None
#     except Exception as e:
#         st.error(f"An unexpected error occurred while fetching API data for {metric} in {region} for {event_name}: {e}")
#         return None
# 
# 
# # ---------------------------------------------------------
# # Modified function: generate_event_tables
# # ---------------------------------------------------------
# def generate_event_tables(events: list[dict], metrics: list[str], countries: list[str]) -> dict[str, pd.DataFrame]:
#     sheets: dict[str, pd.DataFrame] = {}
#     api_relevant_metrics = ["Video Views (VOD)", "Hours Watched (Streams)"] # Define which metrics come from API
# 
#     # Prepare the basic template structure first
#     # This avoids redundant creation inside the inner loops
#     # Note: Column names (baseline_col, actual_col) depend on the event date.
#     # We'll create the DataFrame within the event loop to get correct column names.
# 
#     for ev in events:
#         baseline_col, actual_col = format_span_labels(ev["date"])
# 
#         # Create the DataFrame for this specific event, setting column names correctly
#         sheet_df_data = []
#         for metric in metrics:
#             sheet_df_data.append({
#                 "Metric": metric,
#                 baseline_col: None,
#                 actual_col: None, # Default to None
#                 "Baseline Method": None,
#             })
#         template_df = pd.DataFrame(sheet_df_data)
# 
# 
#         for country in countries:
#             sheet_name = f"{ev['name'][:25] or 'Event'}_{country}"
# 
#             # Start with a copy of the event's base template (which has the right column names)
#             sheet_df = template_df.copy()
# 
#             # Iterate through the metrics selected for this sheet
#             for i, metric in enumerate(sheet_df["Metric"]):
#                  # Check if this metric is one of the API relevant ones AND is selected by the user
#                  if metric in api_relevant_metrics and metric in metrics:
#                      st.info(f"Processing {metric} for {country} for {ev['name']}...") # Info message before API call
#                      api_value = fetch_data_from_api(ev["name"], ev["date"], country, metric)
#                      if api_value is not None:
#                          # Populate the 'Actual' column for this metric in this sheet
#                          # Use .loc with the column name
#                          sheet_df.loc[sheet_df["Metric"] == metric, actual_col] = api_value
#                      else:
#                          st.warning(f"Could not fetch API data for {metric} in {country} for {ev['name']}. Leaving 'Actual' blank.")
# 
#             sheets[sheet_name] = sheet_df
# 
#     return sheets
# 
# 
# def compute_final_benchmarks(uploaded_file) -> dict[str, pd.DataFrame]:
#     xls = pd.ExcelFile(uploaded_file)
#     sheets_all = {sh: pd.read_excel(xls, sh) for sh in xls.sheet_names}
#     region_groups: dict[str, list[pd.DataFrame]] = {}
# 
#     for name, df in sheets_all.items():
#         parts = name.rsplit("_", 1)
#         if len(parts) != 2:
#             continue
#         region_groups.setdefault(parts[1], []).append(df)
# 
#     summary: dict[str, pd.DataFrame] = {}
#     for region, dfs in region_groups.items():
#         # Ensure metrics_list is consistent across all sheets for this region if possible
#         # Using the metrics from the first DataFrame for simplicity here,
#         # but robust code might merge metrics from all DataFrames.
#         if not dfs or dfs[0].empty:
#             st.warning(f"No dataframes found for region {region} or first dataframe is empty. Skipping summary.")
#             continue
# 
#         metrics_list = dfs[0]["Metric"].tolist()
#         data = {"Metric": metrics_list}
#         avg_actuals, avg_uplift, avg_method = [], [], []
# 
#         for m in metrics_list:
#             acts, upls, meths = [], [], []
#             for df in dfs:
#                 # Dynamically find column names as they include dates
#                 cols = df.columns.tolist()
#                 # Assuming baseline is col 1 and actual is col 2 after Metric
#                 if len(cols) < 3:
#                      continue # Skip if not enough columns
#                 b_col = cols[1]
#                 a_col = cols[2]
#                 meth_col = cols[3] if len(cols) > 3 else None
# 
#                 row = df[df["Metric"] == m]
#                 if row.empty:
#                     continue
# 
#                 # Handle potential errors if columns don't exist in a specific sheet or row access issues
#                 try:
#                     base = row[b_col].iloc[0] if b_col in row.columns else None # Use iloc[0] for clarity/safety
#                     act  = row[a_col].iloc[0] if a_col in row.columns else None # Use iloc[0]
#                     meth = row[meth_col].iloc[0] if meth_col and meth_col in row.columns else None # Use iloc[0]
#                 except IndexError: # Catch cases where row might be empty even after filter (unlikely but safe)
#                     st.warning(f"IndexError accessing row for metric {m} in a sheet for region {region}. Skipping.")
#                     continue
#                 except KeyError as e:
#                      st.warning(f"KeyError accessing column {e} for metric {m} in a sheet for region {region}. Skipping.")
#                      continue
# 
# 
#                 if pd.notna(act):
#                     acts.append(act)
# 
#                 if meth is not None and pd.notna(meth): # Check for both None and pandas NaN
#                     meths.append(meth)
# 
#                 if pd.notna(base) and pd.notna(act):
#                     if base != 0: # Avoid division by zero
#                         upls.append((act - base) / base * 100)
#                     elif act > 0:
#                          upls.append(float('inf')) # Infinite uplift if base is 0 and actual > 0
#                     else: # base is 0 and act is 0
#                          upls.append(0.0) # 0 uplift
# 
#             # Compute averages, handling cases with no valid data
#             avg_actuals_val = np.nanmean(acts) if acts else np.nan
#             avg_method_val = np.nanmean(meths) if meths else np.nan # Use nanmean for methods too if they are numerical
#             avg_uplift_val = np.nanmean(upls) if upls else np.nan
# 
#             avg_actuals.append(avg_actuals_val)
#             avg_method.append(avg_method_val)
#             avg_uplift.append(avg_uplift_val)
# 
# 
#         data["Average Actuals"]        = avg_actuals
#         data["Baseline Method"]        = avg_method
#         data["Baseline Uplift Expect"] = avg_uplift
#         # Compute Proposed Benchmark - need to handle potential NaN from avg_actuals and avg_method
#         proposed_benchmark = []
#         for actual, method in zip(avg_actuals, avg_method):
#             valid_values = [v for v in [actual, method] if pd.notna(v)]
#             if valid_values:
#                  proposed_benchmark.append(np.nanmedian(valid_values))
#             else:
#                  proposed_benchmark.append(np.nan)
# 
# 
#         data["Proposed Benchmark"]     = proposed_benchmark
#         summary[region] = pd.DataFrame(data)
# 
#     return summary
# 
# 
# # ---------------------------------------------------------
# # Streamlit App UI
# # ---------------------------------------------------------
# st.set_page_config(page_title="Event Marketing Analytics", layout="wide")
# st.markdown(
#     """
# # 📊 Event Marketing Analytics Suite
# 
# Welcome! This tool helps you:
# 
# 1.  **Generate** a template workbook to track multiple events and regions.
# 2.  **Download** the blank workbook, fill in your data (Baseline, Actual, Baseline Method).
# 3.  **Upload** the completed workbook to compute final benchmarks per region.
# 
# Select a mode below to get started.
# """,
#     unsafe_allow_html=True
# )
# 
# mode = st.sidebar.radio(
#     "Select an action:",
#     ["Generate template", "Final benchmarks"]
# )
# 
# if mode == "Generate template":
#     st.sidebar.header("Step 1: Configure template")
#     st.sidebar.markdown(
#         """
#         Specify your event names, start dates, metrics, and regions.
#         When ready, click **Generate template** to download the Excel file.
#         Metrics marked with * will attempt to fetch data from an API.
#         """
#     )
#     n_events = st.sidebar.number_input("Quantity of events", 1, 20, 1)
#     events: list[dict] = []
#     for i in range(n_events):
#         st.sidebar.subheader(f"Event {i+1}")
#         nm = st.sidebar.text_input("Name", key=f"nm{i}") or f"Event{i+1}"
#         dt = st.sidebar.date_input("Start date (T)", key=f"dt{i}")
#         if isinstance(dt, list):
#             dt = dt[0]
#         events.append({"name": nm, "date": datetime.combine(dt, datetime.min.time())})
# 
#     # Update metric list to indicate API fetched metrics
#     all_metrics = [
#         "Sessions", "DAU", "Revenue", "Installs", "Retention", "Watch Time", "ARPU", "Conversions",
#         "Video Views (VOD)*", "Hours Watched (Streams)*", "Social Mentions", "Search Index",
#         "Broadcast TWT", "PCCV", "AMA"
#     ]
#     metrics_display = st.sidebar.multiselect(
#         "Metrics to measure:",
#         all_metrics,
#         default=["Sessions", "DAU", "Revenue", "Installs", "Video Views (VOD)*", "Hours Watched (Streams)*"] # Include API metrics by default
#     )
#     # Clean up metric names for internal logic (remove the '*')
#     metrics = [m.replace("*", "") for m in metrics_display]
# 
# 
#     countries = st.sidebar.multiselect(
#         "Regions/Countries:",
#         ["US","GB","KR","JP","BR","DE","AU","CA","FR","IN","TH","ID","SA"],
#         default=["US","GB","AU"]
#     )
#     if st.sidebar.button("Generate template 📥"):
#         if not events or not metrics or not countries:
#              st.warning("Please configure at least one event, metric, and country.")
#         else:
#             # Add a spinner while generating template (includes API calls)
#             with st.spinner("Generating template (may take a moment)..."): # Added the colon here
#                 sheets = generate_event_tables(events, metrics, countries)

import os, time
from pyngrok import ngrok

# ── Step 1: Kill any existing Streamlit processes ────────────
os.system("pkill -f streamlit")
time.sleep(1)

# ── Step 2: Start Streamlit in the background via python -m ────
cmd = (
    "nohup python3 -m streamlit run event_marketing_app.py "
    "--server.port 8501 "
    "--server.headless true "
    "--server.address 0.0.0.0 "
    "--server.enableCORS false "
    "--server.enableXsrfProtection false "
    "> streamlit.log 2>&1 &"
)
os.system(cmd)
time.sleep(5)  # wait for Streamlit to spin up

# ── Optional debug prints ─────────────────────────────────────
print("❓ Streamlit processes:")
os.system("ps -ax | grep '[s]treamlit' || echo '→ none'")

print("\n❓ Ports listening on 8501:")
os.system("netstat -nlp 2>/dev/null | grep 8501 || echo '→ nothing listening'")

print("\n📄 First 10 lines of streamlit.log:")
os.system("head -n 10 streamlit.log || echo '→ streamlit.log missing'")

# ── Step 3: Open (or reuse) a single ngrok tunnel ─────────────
tunnels = ngrok.get_tunnels()
if tunnels:
    public_url = tunnels[0].public_url
else:
    # Ensure the ngrok authtoken is configured in the first cell
    # If not, this line will fail.
    try:
        public_url = ngrok.connect(8501, bind_tls=True).public_url
    except Exception as e:
        print(f"Error creating ngrok tunnel: {e}")
        print("Please ensure you have configured your ngrok authtoken in the first cell.")
        public_url = "Ngrok tunnel failed to start."


print(f"\n🌐 Your Streamlit app is live at: {public_url}")